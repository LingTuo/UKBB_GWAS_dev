{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "86a0d175-91b5-464f-87cb-1133233b95a9",
   "metadata": {
    "kernel": "SoS"
   },
   "source": [
    "# Scalable pipeline for computing LD matrix in big sample phenotype"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f0b305a-a984-4ae4-9f94-4471336c3844",
   "metadata": {
    "kernel": "SoS"
   },
   "source": [
    "## Aim"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "722c2c61-2a92-43ee-94ae-26bcb892dac6",
   "metadata": {
    "kernel": "SoS"
   },
   "source": [
    "To extract the summary statistics and genotype on specific genomic regions and calculate their LD matrix."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "842f256a-a996-4277-a450-49a14467db6f",
   "metadata": {
    "kernel": "SoS"
   },
   "source": [
    "## Pre-requisites\n",
    "\n",
    "### Two way to use this pipelin in csglogin\n",
    "\n",
    "`export PATH=/home/yh3455/miniconda3/bin:$PATH`\n",
    "\n",
    "### Or insatll the following packages in your env\n",
    "\n",
    "Make sure you install the pre-requisited before running this notebook:\n",
    "\n",
    "```\n",
    "pip install scipy\n",
    "pip install torch\n",
    "pip install dask\n",
    "pip install liftover\n",
    "pip install pandas-plink\n",
    "pip install bgen-reader\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c3b9fe0-fa9a-4daa-99ac-397184f1e239",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "kernel": "SoS",
    "tags": []
   },
   "source": [
    "### Input\n",
    "\n",
    "- `--region-file`, including a list of regions\n",
    "    - Each locus will be represented by one line in the region file with 3 columns chr, start, and end. e.g. `7 27723990 28723990`\n",
    "- `--geno-path`, the path of a genotype inventory, which lists the path of all genotype file in `bgen` format or in `plink` format.\n",
    "    - The list is a file with 2 columns: `chr genotype_file_chr.ext`. \n",
    "    - The first column is chromosome ID, the 2nd file is genotype for that chromosome.\n",
    "    - When chromosome ID is 0, it implies that the genotype file contains all the genotypes.\n",
    "- `--pheno-path`, the path of a phenotype. Only for one genotype data. If `None`, only `pld` will be calculated.\n",
    "    - The phenotype file should have a column with the name `IID`, which is used to represent the sample ID.\n",
    "- `--sumstats-path`, the path of the GWAS file, including all summary statistics (eg, $\\hat{\\beta}$, $SE(\\hat{\\beta})$ and p-values)\n",
    "    - These summary statistics should contain at least these columns: `chrom, pos, ref, alt, snp_id, bhat, sbhat, p`\n",
    "- `--unrelated-samples`, the file path of unrelated samples with a column named `IID`. If `None`, all samples will be considered unrelative.  \n",
    "- `--cwd`, the path of output directory\n",
    "\n",
    "\n",
    "- `--imp-geno-path`, the path of a genotype inventory, which lists the path of all genotype file in `bgen` format or in `plink` format.\n",
    "    - The list is a file with 2 columns: `chr genotype_file_chr.ext`. \n",
    "    - The first column is chromosome ID, the 2nd file is genotype for that chromosome.\n",
    "    - When chromosome ID is 0, it implies that the genotype file contains all the genotypes.\n",
    "- `--imp-sumstats-path`, the path of the GWAS file, including all summary statistics (eg, $\\hat{\\beta}$, $SE(\\hat{\\beta})$ and p-values)\n",
    "    - These summary statistics should contain at least these columns: `chrom, pos, ref, alt, snp_id, bhat, sbhat, p`\n",
    "- `--imp-ref`, the reference genome if exome genotype and imputed genotype are different. If `None`, The two genotype data will be considered from the same  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8cab052-37c8-43c0-9c92-dabc7ebf273d",
   "metadata": {
    "kernel": "SoS",
    "tags": []
   },
   "source": [
    "### Output\n",
    "- `rg_stat`, the reginonal summary stats\n",
    "    - The rowname is the variant ID.\n",
    "    - It should contain at least the following columns: `CHR, BP, SNP, ALT, REF, BETA, SE, Z, P`.\n",
    "- `rg_geno`,the regional genotypes\n",
    "    - The rowname is the variant ID, which should match with the rowname of `rg_stat`.\n",
    "    - The column name is the sample's IID, which is sorted by the sample in phenotype.\n",
    "- `pld`, the regional approximate population LD calculated by unrelated individuals\n",
    "- `sld`, the regional approximate sample LD calcualted by unrelated individuals in a phenotype."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1652355-9cf9-4065-b07e-f763bdb4cc20",
   "metadata": {
    "kernel": "SoS"
   },
   "source": [
    "## Workflow codes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e9c7a420-d78d-4203-b33b-13b5d807a6e0",
   "metadata": {
    "kernel": "SoS",
    "tags": []
   },
   "outputs": [],
   "source": [
    "[global]\n",
    "# Work directory where output will be saved to\n",
    "parameter: cwd = path\n",
    "# Region specifications\n",
    "parameter: region_file = path\n",
    "# Genotype file inventory\n",
    "parameter: geno_path = path\n",
    "# Phenotype path\n",
    "parameter: pheno_path = path\n",
    "# Sample file path, for bgen format\n",
    "parameter: bgen_sample_path = path('.')\n",
    "# Path to summary stats file\n",
    "parameter: sumstats_path = path\n",
    "# Path to summary stats format configuration\n",
    "parameter: format_config_path = path('.')\n",
    "# Path to samples of unrelated individuals\n",
    "parameter: unrelated_samples = path\n",
    "# Number of tasks to run in each job on cluster\n",
    "parameter: job_size = int\n",
    "# Number of tasks to run in each job on cluster\n",
    "parameter: imp_geno_path = path\n",
    "# Path to summary stats file\n",
    "parameter: imp_sumstats_path = path\n",
    "# The reference genome of imputed genotype data\n",
    "parameter: imp_ref = str\n",
    "\n",
    "fail_if(not region_file.is_file(), msg = 'Cannot find regions to extract. Please specify them using ``--region-file`` option.')\n",
    "# Load all regions of interest. Each item in the list will be a region: (chr, start, end)\n",
    "regions = list(set([tuple(x.strip().split()) for x in open(region_file).readlines() if x.strip()]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "176447dd-af7b-458c-af2d-71fe698558a5",
   "metadata": {
    "kernel": "SoS",
    "tags": []
   },
   "outputs": [],
   "source": [
    "[default_1 (export utils script)]\n",
    "depends: Py_Module('torch'), Py_Module('numpy'), Py_Module('pandas'), Py_Module('dask'), Py_Module('scipy'), Py_Module('bgen_reader'), Py_Module('pandas_plink'), Py_Module('liftover'), Py_Module('xxhash')\n",
    "parameter: scan_window = 500000\n",
    "output: f'{cwd:a}/utils.py'\n",
    "report: expand = '${ }', output=f'{cwd:a}/utils.py'\n",
    "    import torch\n",
    "    import numpy as np\n",
    "    import pandas as pd\n",
    "    from scipy.stats import norm\n",
    "    import dask.array as da\n",
    "    import dask.dataframe as dd\n",
    "    from bgen_reader import open_bgen  \n",
    "    from pandas_plink import read_plink\n",
    "    from liftover import get_lifter\n",
    "    from xxhash import xxh32 as xxh\n",
    "\n",
    "    #functions to read sumstats\n",
    "    def read_sumstat(file, config_file):\n",
    "        try:\n",
    "            sumstats = pd.read_csv(file, compression='gzip', header=0, sep='\\t', quotechar='\"')\n",
    "        except:\n",
    "            sumstats = pd.read_csv(file, header=0, sep='\\t', quotechar='\"')\n",
    "        if config_file is not None:\n",
    "            import yaml\n",
    "            config = yaml.safe_load(open(config_file, 'r'))\n",
    "            try:\n",
    "                sumstats = sumstats.loc[:,list(config.values())]\n",
    "            except:\n",
    "                raise ValueError(f'According to {config_file}, input summary statistics should have the following columns: {list(config.values())}.')\n",
    "            sumstats.columns = list(config.keys())\n",
    "        sumstats.SNP = sumstats.SNP.apply(shorten_id)\n",
    "        sumstats.CHR = sumstats.CHR.astype(int)\n",
    "        sumstats.POS = sumstats.POS.astype(int)\n",
    "        return sumstats\n",
    "\n",
    "    def read_regenie(file):\n",
    "        try:\n",
    "            sumstats = pd.read_csv(file, compression='gzip', header=0, sep='\\t', quotechar='\"')\n",
    "        except:\n",
    "            sumstats = pd.read_csv(file, header=0, sep='\\t', quotechar='\"')\n",
    "        sumstats.SNP = 'chr'+sumstats.CHR.astype(str) + ':' + sumstats.POS.astype(str) + ':' + sumstats.REF.astype(str) + ':' + sumstats.ALT.astype(str)\n",
    "        sumstats.CHR = sumstats.CHR.astype(int)\n",
    "        sumstats.POS = sumstats.POS.astype(int)\n",
    "        return sumstats\n",
    "\n",
    "    #util functions\n",
    "    def shorten_id(x):\n",
    "        return x if len(x) < 30 else f\"{x.split('_')[0]}_{xxh(x).hexdigest()}\"\n",
    "\n",
    "    def regional_stats(sumstats, region):\n",
    "        ss = sumstats[(sumstats.CHR == region[0]) & (sumstats.POS >= region[1]) & (sumstats.POS <= region[2])].copy()\n",
    "        ss['Z'] = list(p2z(ss.P,ss.BETA))\n",
    "        return ss\n",
    "\n",
    "\n",
    "    def p2z(pval,beta,twoside=True):\n",
    "        if twoside:\n",
    "            pval = pval/2\n",
    "        z=np.abs(norm.ppf(pval))\n",
    "        ind=beta<0\n",
    "        z[ind]=-z[ind]\n",
    "        return z\n",
    "\n",
    "\n",
    "    #functions to read genotype data\n",
    "    def read_bgen(file, sample_file=None):\n",
    "        bg = open_bgen(file,verbose=False)\n",
    "        snp,aa0,aa1 = [],[],[]\n",
    "        for c,p,alleles in zip(bg.chromosomes,bg.positions,bg.allele_ids):\n",
    "            a0,a1 = alleles.split(',')\n",
    "            aa0.append(a0)\n",
    "            aa1.append(a1)\n",
    "            snp.append(':'.join(['chr'+str(int(c)),str(p),a1,a0]))\n",
    "        bg.bim = pd.DataFrame({'chrom':bg.chromosomes.astype(int),'snp':snp,'pos':bg.positions,'a0':aa0,'a1':aa1})\n",
    "        if sample_file is not None:\n",
    "            fam = pd.read_csv(sample_file, header=0, delim_whitespace=True, quotechar='\"',skiprows=1)\n",
    "            fam.columns = ['fid','iid','missing','sex']\n",
    "            bg.fam = fam\n",
    "        return bg\n",
    "\n",
    "    def read_pl(file):\n",
    "        (bim,fam,bed) = read_plink(file, verbose=False)\n",
    "        geno = bed\n",
    "        geno.bim = bim\n",
    "        geno.fam = fam\n",
    "        return geno\n",
    "\n",
    "    def read_geno(geno_file):\n",
    "        if geno_file.endswith('.bed'):\n",
    "            geno = read_pl(geno_file[:-4])\n",
    "        elif geno_file.endswith('.bgen'):\n",
    "            sample_file = geno_file.replace('.bgen', '.sample')\n",
    "            geno = read_bgen(geno_file,sample_file)\n",
    "        else:\n",
    "            raise ValueError('Plesae provide the genotype files with PLINK binary format or BGEN format')\n",
    "        return geno\n",
    "\n",
    "    #The function to find an overlap region between geno data with sumstat\n",
    "    def geno_in_stat(geno,stat,notin=False):\n",
    "        bim = geno.bim\n",
    "        fam = geno.fam\n",
    "        idx = bim.snp.isin(stat.SNP)\n",
    "        if notin:\n",
    "            idx = idx == False\n",
    "        else:\n",
    "            if sum(idx)!=stat.shape[0]:\n",
    "                print(\"error mismatch between geno and stat\")\n",
    "        if isinstance(geno,da.core.Array):\n",
    "            geno = geno[idx,:]\n",
    "        else:\n",
    "            int_idx = list(idx[idx].index)\n",
    "            geno = bgen2dask(geno,int_idx,step=500).T\n",
    "        geno.bim = bim[idx]\n",
    "        geno.fam = fam\n",
    "        return geno\n",
    "\n",
    "    #The function to covert bgen to dask array\n",
    "    def bgen2dask(bgen,index,step=500):\n",
    "        genos = []\n",
    "        n = len(index)\n",
    "        for i in range(0,n,step):\n",
    "            onecode_geno = bgen.read(index[i:min(n,i+step)])\n",
    "            geno = onecode_geno.argmax(axis=2).astype(np.int8)\n",
    "            genos.append(da.from_array(geno))\n",
    "        return(da.concatenate(genos,axis=1))\n",
    "\n",
    "    #The function to liftover bim\n",
    "    def bim_liftover(bim,chainmap):\n",
    "        new_bim = []\n",
    "        for c,p,a0,a1 in zip(bim.chrom,bim.pos,bim.a0,bim.a1):\n",
    "            new_c,new_p,_ = chainmap[int(c)][p][0]\n",
    "            snp = ':'.join([new_c,str(new_p),a0,a1])\n",
    "            new_bim.append([int(new_c[3:]),snp,new_p,a0,a1])\n",
    "        new_bim = pd.DataFrame(new_bim,columns=['chrom','snp','pos','a0','a1'])\n",
    "        return new_bim\n",
    "\n",
    "    #The function to find an overlap samples between geno data with unr\n",
    "    def geno_in_unr(geno,unr):\n",
    "        bim = geno.bim\n",
    "        fam = geno.fam\n",
    "        idx = fam.iid.astype(str).isin(unr.IID.astype(str))\n",
    "        geno = geno[:,idx]\n",
    "        geno.bim = bim\n",
    "        geno.fam = fam[idx]\n",
    "        return geno\n",
    "\n",
    "    #functions to calculate LD matrix\n",
    "    def geno_LD(x,y=None,step=100):\n",
    "        if y is None:\n",
    "            dd = dask_corr(x,step)\n",
    "            return(dict2mat(dd))\n",
    "        else:\n",
    "            dd = dask_corr_pair(x,y,step)\n",
    "            return(dict2mat_pair(dd))\n",
    "\n",
    "    def dask_corr(genos,step=100):\n",
    "        #sample by snps (normalized)\n",
    "        nsample = genos.shape[0]\n",
    "        nsnp = genos.shape[1]\n",
    "        da_corr = {}\n",
    "        for i in range(0,nsnp,step):\n",
    "            da_corr[i] = {}\n",
    "            geno_i = genos[:,i:min(i+step,nsnp)].compute().astype(np.float64)\n",
    "            geno_i = (geno_i - np.nanmean(geno_i,axis=0)[None,:])/np.nanstd(geno_i,axis=0)[None,:]\n",
    "            geno_i = torch.from_numpy(geno_i)\n",
    "            geno_i[torch.isnan(geno_i)] = 0\n",
    "            chunk_i = da.from_array((torch.matmul(geno_i.T,geno_i)/nsample).numpy())\n",
    "            da_corr[i][i]=chunk_i\n",
    "            for j in range(i+step,nsnp,step):\n",
    "                geno_j = genos[:,j:min(j+step,nsnp)].compute().astype(np.float64)\n",
    "                geno_j = (geno_j - np.nanmean(geno_j,axis=0)[None,:])/np.nanstd(geno_j,axis=0)[None,:]\n",
    "                geno_j = torch.from_numpy(geno_j)\n",
    "                geno_j[torch.isnan(geno_j)] = 0\n",
    "                cor_ij = da.from_array((torch.matmul(geno_i.T,geno_j)/nsample).numpy())\n",
    "                da_corr[i][j]=cor_ij\n",
    "        return da_corr\n",
    "\n",
    "    def dict2mat(dd):\n",
    "        da_mat=[]\n",
    "        for i in dd.keys():\n",
    "            rowi = []\n",
    "            for j in dd.keys():\n",
    "                if i>j:\n",
    "                    rowi.append(dd[j][i].T)\n",
    "                else:\n",
    "                    rowi.append(dd[i][j])\n",
    "            rowi = da.concatenate(rowi,axis=1)\n",
    "            da_mat.append(rowi)\n",
    "        return(da.concatenate(da_mat,axis=0))\n",
    "\n",
    "    def dask_corr_pair(genos,pgenos,step=100):\n",
    "        #sample by snps (normalized)\n",
    "        nsample = genos.shape[0]\n",
    "        nsnp = genos.shape[1]\n",
    "        psample = pgenos.shape[0]\n",
    "        psnp = pgenos.shape[1]\n",
    "        if nsample != psample: print(\"error: sample not match\")\n",
    "        da_corr = {}\n",
    "        for i in range(0,nsnp,step):\n",
    "            da_corr[i] = {}\n",
    "            geno_i = genos[:,i:min(i+step,nsnp)].compute().astype(np.float64)\n",
    "            geno_i = (geno_i - np.nanmean(geno_i,axis=0)[None,:])/np.nanstd(geno_i,axis=0)[None,:]\n",
    "            geno_i = torch.from_numpy(geno_i)\n",
    "            geno_i[torch.isnan(geno_i)] = 0\n",
    "            for j in range(0,psnp,step):\n",
    "                geno_j = pgenos[:,j:min(j+step,psnp)].compute().astype(np.float64)\n",
    "                geno_j = (geno_j - np.nanmean(geno_j,axis=0)[None,:])/np.nanstd(geno_j,axis=0)[None,:]\n",
    "                geno_j = torch.from_numpy(geno_j)\n",
    "                geno_j[torch.isnan(geno_j)] = 0\n",
    "                cor_ij = da.from_array((torch.matmul(geno_i.T,geno_j)/nsample).numpy())\n",
    "                da_corr[i][j]=cor_ij\n",
    "        return da_corr\n",
    "\n",
    "    def dict2mat_pair(dd):\n",
    "        da_mat=[]\n",
    "        for i in dd.keys():\n",
    "            rowi = []\n",
    "            for j in dd[0].keys():\n",
    "                rowi.append(dd[i][j])\n",
    "            rowi = da.concatenate(rowi,axis=1)\n",
    "            da_mat.append(rowi)\n",
    "        return(da.concatenate(da_mat,axis=0))\n",
    "\n",
    "    def main(region,geno_path,sumstats_path,pheno_path,unr_path,imp_geno_path,imp_sumstats_path,imp_ref,output_sumstats,output_LD):\n",
    "\n",
    "        print('1. Preprocess sumstats (regenie format) and extract it from a region')\n",
    "        if pheno_path is not None:\n",
    "            # Load phenotype file\n",
    "            pheno = pd.read_csv(pheno_path, header=0, delim_whitespace=True, quotechar='\"')\n",
    "        if unr_path is not None:\n",
    "            # Load unrelated sample file\n",
    "            unr = pd.read_csv(unr_path, header=0, delim_whitespace=True, quotechar='\"')  \n",
    "        # Load the file of summary statistics and standardize it.\n",
    "        exome_sumstats = read_regenie(sumstats_path)\n",
    "        exome_geno = read_geno(geno_path)\n",
    "        print('1.1. Region extraction')\n",
    "        exome_sumstats = regional_stats(exome_sumstats,region)\n",
    "        exome_geno = geno_in_stat(exome_geno,exome_sumstats)\n",
    "\n",
    "        if imp_geno_path is not None:\n",
    "            #two genotype data\n",
    "            imput_sumstats = read_regenie(imp_sumstats_path)\n",
    "            imput_geno = read_geno(imp_geno_path)   \n",
    "            if imp_ref is None:\n",
    "                imput_sumstats = regional_stats(imput_sumstats,region)\n",
    "                imput_geno = geno_in_stat(imput_geno,imput_sumstats)\n",
    "            else:\n",
    "                print('1.2. LiftOver the region')\n",
    "                from liftover import get_lifter\n",
    "                hg38toimpref = get_lifter('hg38',imp_ref)\n",
    "                imp_start = hg38toimpref[region[0]][region[1]][0][1]\n",
    "                imp_end = hg38toimpref[region[0]][region[2]][0][1]\n",
    "                imp_region = [region[0],imp_start,imp_end]\n",
    "                imput_sumstats = regional_stats(imput_sumstats,imp_region)\n",
    "                imput_geno = geno_in_stat(imput_geno,imput_sumstats)\n",
    "                print('1.3. Regional SNPs Liftover')\n",
    "                impreftohg38 = get_lifter(imp_ref,'hg38') #oppsite with hg38toimpref\n",
    "                imput_geno.bim = bim_liftover(imput_geno.bim,impreftohg38)\n",
    "                imput_sumstats.POS = list(imput_geno.bim.pos)\n",
    "                imput_sumstats.SNP = list(imput_geno.bim.snp)\n",
    "            print('1.1.1 Get exome unique sumstats and geno and Combine sumstats')\n",
    "            exome_unique_snp_idx = exome_sumstats.SNP.isin(imput_sumstats.SNP)==False\n",
    "            exome_sumstats_diff = exome_sumstats[exome_unique_snp_idx]\n",
    "            sumstats = pd.concat([exome_sumstats_diff,imput_sumstats])\n",
    "            exome_geno = geno_in_stat(exome_geno,imput_sumstats,notin=True)\n",
    "        else:\n",
    "            #one genotype data\n",
    "            sumstats = exome_sumstats\n",
    "\n",
    "        print('2. Remove relative samples')\n",
    "        if unr_path is not None:\n",
    "            exome_geno = geno_in_unr(exome_geno,unr)\n",
    "            if imp_geno_path is not None:\n",
    "                imput_geno = geno_in_unr(imput_geno,unr)\n",
    "        else:\n",
    "            print('Warning:There is no file of relative sample. All sample are included in computing LD matrix')\n",
    "\n",
    "        if pheno_path is not None:\n",
    "            pass #sld and pld\n",
    "\n",
    "        print('3. Calculate LD matrix')\n",
    "        if imp_geno_path is None:\n",
    "            cor_da = geno_LD(exome_geno.T)\n",
    "        else:\n",
    "            xx = geno_LD(exome_geno.T)\n",
    "            yy = geno_LD(imput_geno.T,step=500)\n",
    "\n",
    "            imput_fam = imput_geno.fam\n",
    "            imput_fam.index = list(imput_fam.iid.astype(str))\n",
    "            imput_fam['i'] = list(range(imput_fam.shape[0]))\n",
    "            imput_fam_comm = imput_fam.loc[list(exome_geno.fam.iid.astype(str))]\n",
    "            imput_geno_comm=imput_geno[:,list(imput_fam_comm.i)]\n",
    "            xy = geno_LD(exome_geno.T,imput_geno_comm.T,step=500)\n",
    "            cor_da = da.concatenate([da.concatenate([xx,xy],axis=1),da.concatenate([xy.T,yy],axis=1)],axis=0)\n",
    "\n",
    "        print('4. Output sumstats and LD matrix')\n",
    "        index = list(sumstats.SNP.apply(shorten_id))\n",
    "        sumstats.SNP = index\n",
    "        sumstats.index = list(range(sumstats.shape[0]))\n",
    "        sumstats.to_csv(output_sumstats, sep = \"\\t\", header = True, index = True)\n",
    "\n",
    "        corr = cor_da.compute()\n",
    "        np.fill_diagonal(corr, 1)\n",
    "        corr = pd.DataFrame(corr, columns=index)\n",
    "        corr.to_csv(output_LD, sep = \"\\t\", header = True, index = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba29e4c6-b5b8-41bb-b08d-f31da8f2c85f",
   "metadata": {
    "kernel": "SoS"
   },
   "source": [
    "## Extract data\n",
    "\n",
    "This step runs in parallel for all loci listed in the region file (via `for_each`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a9e31014-3435-48ea-bd15-049da3da5a72",
   "metadata": {
    "kernel": "SoS"
   },
   "outputs": [],
   "source": [
    "[default_2 (extract genotypes)]\n",
    "depends: f'{cwd:a}/utils.py'\n",
    "input: geno_path, pheno_path, sumstats_path, unrelated_samples, imp_geno_path,imp_sumstats_path,imp_ref, for_each = 'regions'\n",
    "output: sumstats = f'{cwd:a}/{_regions[0]}_{_regions[1]}_{_regions[2]}/{sumstats_path:bn}_{_regions[0]}_{_regions[1]}_{_regions[2]}.sumstats.gz',\n",
    "        genotype = f'{cwd:a}/{_regions[0]}_{_regions[1]}_{_regions[2]}/{sumstats_path:bn}_{_regions[0]}_{_regions[1]}_{_regions[2]}.genotype.gz',\n",
    "        pld = f'{cwd:a}/{_regions[0]}_{_regions[1]}_{_regions[2]}/{sumstats_path:bn}_{_regions[0]}_{_regions[1]}_{_regions[2]}.pre_pop_ld.pickle',\n",
    "        sld = f'{cwd:a}/{_regions[0]}_{_regions[1]}_{_regions[2]}/{sumstats_path:bn}_{_regions[0]}_{_regions[1]}_{_regions[2]}.pre_sample_ld.pickle'\n",
    "task: trunk_workers = 1, trunk_size = job_size, walltime = '4h', mem = '60G', cores = 1, tags = f'{step_name}_{_output[0]:bn}'\n",
    "python: expand = '${ }', input = f'{cwd:a}/utils.py', stderr = f'{_output[0]:n}.stderr', stdout = f'{_output[0]:n}.stdout'\n",
    "    \n",
    "\n",
    "    import os\n",
    "    # output path files that we will need in our final version\n",
    "    output_sumstats = ${_output['sumstats']:r}\n",
    "    output_genotype = ${_output['genotype']:r}\n",
    "    output_pld = ${_output['pld']:r}\n",
    "    output_sld = ${_output['sld']:r}\n",
    "\n",
    "    # this general path is used to create other temporary files that we need to calculate the ld matrices later on\n",
    "    cwd = os.getcwd()\n",
    "    output_general = '${cwd}/${_regions[0]}_${_regions[1]}_${_regions[2]}/${sumstats_path:bn}_${_regions[0]}_${_regions[1]}_${_regions[2]}'\n",
    "\n",
    "    input_sample_path = ${bgen_sample_path:r}\n",
    "    input_geno_path = ${_input[0]:r}\n",
    "    input_pheno_path = ${_input[1]:r}\n",
    "    input_sumstats_path = ${_input[2]:r}\n",
    "    input_unrelated_samples = ${_input[3]:r}\n",
    "    imp_geno_path = ${_input[4]:r}\n",
    "    imp_sumstats_path = ${_input[5]:r}\n",
    "    imp_ref =  ${_input[6]:r}\n",
    "    \n",
    "    input_format_config = ${format_config_path:r} if ${format_config_path.is_file()} else None\n",
    "\n",
    "    \n",
    "    # Load genotype file for the region of interest\n",
    "    geno_inventory = dict([x.strip().split() for x in open(${_input[0]:r}).readlines() if x.strip()])\n",
    "    chrom = \"${_regions[0]}\"\n",
    "    if chrom.startswith('chr'):\n",
    "        chrom = chrom[3:]\n",
    "    if chrom not in geno_inventory:\n",
    "        geno_file = geno_inventory['0']\n",
    "    else:\n",
    "        geno_file = geno_inventory[chrom]\n",
    "\n",
    "    print(geno_file, input_sumstats_path, input_pheno_path, input_unrelated_samples,output_sumstats, output_pld)\n",
    "\n",
    "\n",
    "    if not os.path.isfile(geno_file):\n",
    "        # relative path\n",
    "        if not os.path.isfile('${_input[0]:ad}/' + geno_file):\n",
    "            raise ValueError(f\"Cannot find genotype file {geno_file}\")\n",
    "        else:\n",
    "            geno_file = '${_input[0]:ad}/' + geno_file\n",
    "\n",
    "\n",
    "    region = (int(chrom), ${_regions[1]}, ${_regions[2]})\n",
    "\n",
    "    imput_pheno_path = None\n",
    "    imp_geno_path,imp_sumstats_path,imp_ref = None,None,None\n",
    "    print(region, geno_file, input_sumstats_path, input_pheno_path, input_unrelated_samples,imp_geno_path,imp_sumstats_path,imp_ref,\n",
    "                                output_sumstats, output_pld)\n",
    "    main(region, geno_file, input_sumstats_path, input_pheno_path, input_unrelated_samples,imp_geno_path,imp_sumstats_path,imp_ref,\n",
    "                                output_sumstats, output_pld)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "044f2079-e68a-483f-a4ca-3e0a3ef4e260",
   "metadata": {
    "kernel": "SoS"
   },
   "source": [
    "region = [5,272741,1213528-900000]\n",
    "geno_path = 'MWE_region_extraction/ukb23156_c5.merged.filtered.5_272741_1213528.bed'\n",
    "sumstats_path = 'MWE_region_extraction/090321_UKBB_Hearing_aid_f3393_expandedwhite_6436cases_96601ctrl_PC1_2_f3393.regenie.snp_stats'\n",
    "pheno_path = None\n",
    "unr_path = 'MWE_region_extraction/UKB_genotypedatadownloaded083019.090221_sample_variant_qc_final_callrate90.filtered.extracted.white_europeans.filtered.092821_ldprun_unrelated.filtered.prune.txt'\n",
    "imp_geno_path = 'MWE_region_extraction/ukb_imp_chr5_v3_05_272856_1213643.bgen'\n",
    "imp_sumstats_path = 'MWE_region_extraction/100521_UKBB_Hearing_aid_f3393_expandedwhite_15601cases_237318ctrl_500k_PC1_PC2_f3393.regenie.snp_stats'\n",
    "imp_ref = 'hg19'\n",
    "\n",
    "output_sumstats = 'test.snp_stats'\n",
    "output_LD = 'test_corr.csv'\n",
    "\n",
    "main(region,geno_path,sumstats_path,pheno_path,unr_path,imp_geno_path,imp_sumstats_path,imp_ref,output_sumstats,output_LD)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a8aed81-d2bd-4adb-8344-7aa80eff32a5",
   "metadata": {
    "kernel": "SoS"
   },
   "source": [
    " sos run /home/yh3455/Github/bioworkflows/GWAS/LD_merged_exo_imp.ipynb     default    --cwd /home/yh3455/Github/bioworkflows/GWAS/test    --region-file /home/dmc2245/UKBiobank/results/LD_clumping/092321_f3393_200Kexomes/090321_UKBB_Hearing_aid_f3393_expandedwhite_6436cases_96601ctrl_PC1_2_f3393.regenie.snp_stats.clumped_region    --pheno-path /home/dmc2245/UKBiobank/phenotype_files/hearing_impairment/090321_UKBB_Hearing_aid_f3393_expandedwhite_6436cases_96601ctrl_PC1_2.tsv    --geno-path /home/dmc2245/UKBiobank/data/exome_files/project_VCF/072721_run/plink/092321_UKBB_qc_exome_geno_path.txt   --sumstats-path /home/dmc2245/UKBiobank/results/REGENIE_results/results_exome_data/090921_f3393_hearing_aid_200K/*.snp_stats.gz     --unrelated-samples /home/dmc2245/UKBiobank/results/083021_PCA_results/090221_ldprun_unrelated/cache/UKB_genotypedatadownloaded083019.090221_sample_variant_qc_final_callrate90.filtered.extracted.europeans.filtered.090221_ldprun_unrelated.filtered.prune.txt  --    --job-size 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67648866-51b6-4903-be54-77b034c1808f",
   "metadata": {
    "kernel": "SoS"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "SoS",
   "language": "sos",
   "name": "sos"
  },
  "language_info": {
   "codemirror_mode": "sos",
   "file_extension": ".sos",
   "mimetype": "text/x-sos",
   "name": "sos",
   "nbconvert_exporter": "sos_notebook.converter.SoS_Exporter",
   "pygments_lexer": "sos"
  },
  "sos": {
   "kernels": [
    [
     "SoS",
     "sos",
     "",
     "",
     "sos"
    ]
   ],
   "version": "0.22.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
