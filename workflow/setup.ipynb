{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "kernel": "SoS"
   },
   "source": [
    "# Setting up the computer environment\n",
    "\n",
    "Notes and tips on software configuration and management on Yale HPC cluster."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "kernel": "SoS"
   },
   "source": [
    "# 1. Accessing Yale HPC cluster"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "kernel": "SoS"
   },
   "source": [
    "Using AnyConnect to establish connection:\n",
    "\n",
    "* VNP: access.yale.edu\n",
    "* username: dc2325\n",
    "* MFA: push # after this accept the access in the duo mobile app\n",
    "\n",
    "Using Linux terminal command:\n",
    "```\n",
    "sudo openconnect -u dc2325 access.yale.edu\n",
    "```\n",
    "Then type password at first password prompt, and `push` at 2nd password prompt. After this accept the access in the duo mobile app.\n",
    "\n",
    "Before you can login you must provide the public key of your computer to the server. To do so, please visit: https://secure.its.yale.edu/cas/login to login, then provide the key at http://gold.hpc.yale.internal/cgi-bin/sshkeys.py\n",
    "\n",
    "To login from the terminal:\n",
    "\n",
    "```\n",
    "ssh dc2325@farnam.hpc.yale.edu\n",
    "```\n",
    "\n",
    "You should not perform any analysis on the login node. If you do, your job may either run into memory error, or get terminated without noticing. However you can submit jobs easily using `sbatch` command. For example instead of running:\n",
    "\n",
    "```\n",
    "# `sos` is our pipeline software\n",
    "sos run analysis.ipynb\n",
    "```\n",
    "\n",
    "you run\n",
    "\n",
    "```\n",
    "sbatch sos run analysis.ipynb\n",
    "```\n",
    "\n",
    "to submit job. This uses default resource allocation on the cluster. The rest of this document provides tips on more advanced job template, job management and logging to an interactive compute node. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "kernel": "Bash"
   },
   "source": [
    "### Loading and listing modules in your environment on the cluster\n",
    "\n",
    "```\n",
    "$ module avail # For a list of modules available to use\n",
    "$ module list # Displays all of the module files that are currently loaded in your environment\n",
    "$ module avail python # To look for specific modules\n",
    "$ module spider # Displays a description of all available modules\n",
    "$ module load <name> # to load pre-installed software\n",
    "$ module unload <name> # to unload\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "kernel": "Bash"
   },
   "source": [
    "### Copying files/directories from and to the cluster"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "kernel": "Bash"
   },
   "source": [
    "To copy from the cluster to your local machine\n",
    "\n",
    "In your local terminal and to copy to the current dir `.`:\n",
    "\n",
    "```\n",
    "scp dc2325@farnam.hpc.yale.edu:/home/dc2325/results/pleiotropy/2020-04_bolt/BMI/*.snp_stats.bgen.gz . \n",
    "scp dc2325@farnam.hpc.yale.edu:/home/dc2325/project/results/pleiotropy/2020-04_bolt/INT-WHR/snp_stats.all_chr.gz . \n",
    "```\n",
    "\n",
    "From your local machine to the cluster:\n",
    "\n",
    "```\n",
    "scp INT_BMI.sumstats.gz dc2325@farnam.hpc.yale.edu:/home/dc2325/scratch60/plink-clumping\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "kernel": "Bash"
   },
   "source": [
    "# 2. Installing software in your $HOME directory"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "kernel": "SoS"
   },
   "source": [
    "## a. Conda installation\n",
    "\n",
    "You'd better install your own python, R, R packages and other softwares needed using miniconda, not to rely on the cluster.\n",
    "###  1. The first step is to download miniconda3 to your local directory, then `sh` to install.\n",
    "```\n",
    "wget https://repo.anaconda.com/miniconda/Miniconda3-latest-Linux-x86_64.sh\n",
    "sh Miniconda3-latest-Linux-x86_64.sh\n",
    "```\n",
    "### 2. Add miniconda to the PATH\n",
    "\n",
    "By default a `.bashrc` file adding miniconda to the `$PATH` will be created, you can then modify as needed. \n",
    "\n",
    "If it gives you error when running `conda` after the  installation, please check your `.bash_profile` with command `$ cat ~/.bash_profile` to see if conda is added to the path. \n",
    "\n",
    "If not you should copy the code from the `.bashrc` file and add it manually:  \n",
    "\n",
    "    \n",
    "1. Open .bashrc file\n",
    "2. Copy the code between `#make miniconda part of the $PATH` and `# <<< conda initialize <<<`\n",
    "3. Add it to the .bash_profile\n",
    "4. Exit the cluster and re-enter or source .bash_profile\n",
    "\n",
    "### 3. Creating and switching environments with conda\n",
    "\n",
    "Installing python 2.7 and creating a conda environment\n",
    "\n",
    "```\n",
    "conda create --name py2 python=2.7\n",
    "conda activate py2\n",
    "conda deactivate\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "kernel": "Bash"
   },
   "source": [
    "## b. BOLT-LMM installation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "kernel": "Bash"
   },
   "source": [
    "For local installs add these lines to your ~/.bash_profile\n",
    "```\n",
    "# local installs\n",
    "export MY_PREFIX=~/software\n",
    "export PATH=$MY_PREFIX/bin:$PATH\n",
    "export LD_LIBRARY_PATH=$MY_PREFIX/lib:$LD_LIBRARY_PATH\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "kernel": "Markdown"
   },
   "source": [
    "Then install the package:\n",
    "\n",
    "```\n",
    "cd ~/software && mkdir bin lib && \\\n",
    "wget https://data.broadinstitute.org/alkesgroup/BOLT-LMM/downloads/BOLT-LMM_v2.3.4.tar.gz && \\\n",
    "tar -zxvf BOLT-LMM_v2.3.4.tar.gz && \\\n",
    "rm -rf BOLT-LMM_v2.3.4.tar.gz && \\\n",
    "cp BOLT-LMM_v2.3.4/bolt ~/software/bin/ && \\\n",
    "cp BOLT-LMM_v2.3.4/lib/* ~/software/lib/\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "kernel": "Bash"
   },
   "source": [
    "## c. SAIGE installation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "kernel": "R"
   },
   "source": [
    "#### Creating a conda environment\n",
    "\n",
    "As per SAIGE tutorial\n",
    "\n",
    "```\n",
    "conda create -n RSAIGE r-essentials r-base=3.6.1 python=2.7\n",
    "conda activate RSAIGE\n",
    "conda install -c anaconda cmake\n",
    "conda install -c conda-forge gettext lapack r-matrix\n",
    "conda install -c r r-rcpp  r-rcpparmadillo r-data.table r-bh\n",
    "conda install -c conda-forge r-spatest r-rcppeigen r-devtools  r-skat r-rcppparallel r-optparse boost openblas\n",
    "pip3 install cget click\n",
    "conda env export > environment-RSAIGE.yml\n",
    "```\n",
    "\n",
    "Solving some error issues in the installation of SAIGE https://github.com/weizhouUMICH/SAIGE/issues/118\n",
    "\n",
    "```\n",
    "conda create -n RSAIGE r-essentials r-base=3.6.1 python=2.7\n",
    "conda activate RSAIGE\n",
    "conda install -c anaconda cmake boost zlib\n",
    "conda install -c conda-forge gettext lapack r-matrix \n",
    "conda install -c conda-forge r-spatest r-rcppeigen r-devtools r-skat\n",
    "conda install -c conda-forge r-rcpp  r-rcpparmadillo r-data.table r-bh\n",
    "conda install -c conda-forge r-rcppparallel r-optparse\n",
    "pip install cget click\n",
    "```\n",
    "\n",
    "\n",
    "#### Activate conda environment\n",
    "\n",
    "```\n",
    " conda activate RSAIGE\n",
    " FLAGPATH=`which python | sed 's|/bin/python$||'`\n",
    " export LDFLAGS=\"-L${FLAGPATH}/lib\"\n",
    " export CPPFLAGS=\"-I${FLAGPATH}/include\"\n",
    " export LDFLAGS=\"-L/gpfs/ysm/project/dewan/dc2325/conda_envs/RSAIGE/lib\"\n",
    " export CPPFLAGS='-I/gpfs/ysm/project/dewan/dc2325/conda_envs/RSAIGE/include'\n",
    "```\n",
    "\n",
    "#### Intall required R libraries\n",
    "\n",
    "\n",
    "For this part I had to install MetaSKAT using the remotes library otherwise I found an error\n",
    "```\n",
    "install.packages(\"remotes\")\n",
    "remotes::install_github(\"lin-lab/MetaSKAT\")\n",
    "```\n",
    "#### Install SAIGE\n",
    "\n",
    "Method 2: this method did not work for me, so I proceed to the next one\n",
    "\n",
    "```\n",
    "devtools::install_github(\"weizhouUMICH/SAIGE\")\n",
    "```\n",
    "\n",
    "Method 3\n",
    "\n",
    "```\n",
    "src_branch=master\n",
    "repo_src_url=https://github.com/weizhouUMICH/SAIGE\n",
    "git clone --depth 1 -b $src_branch $repo_src_url\n",
    "R CMD INSTALL SAIGE\n",
    "```\n",
    "\n",
    "#### SAIGE on Yale cluster\n",
    "\n",
    "To install SAIGE in the HRC cluster first load necessary modules to create aspecific environment\n",
    "\n",
    "Search and load modules\n",
    "\n",
    "* `module avail` for a list of all available modules\n",
    " \n",
    "* `module avail R` to see a list of all available R modules in Yale's HRC\n",
    "\n",
    "\tSelect R-3.6.1 version if available by typing `module load R-3.6.1`\n",
    "\n",
    "* `module avail gcc`\n",
    "\n",
    "\tSelect gcc >= 5.4.0: `module load gcc-5.4.1`\n",
    "\n",
    "* `module avail cmake`\n",
    "\n",
    "\tSelect cmake 3.14.1: `module load cmake-3.14.1`\n",
    "\n",
    "* `module avail cget`\n",
    "\n",
    "\tSelect the latest version of cget: `module load cget`\n",
    "\n",
    "* Install R packages using the `install_packages.R` script\n",
    "\n",
    "\n",
    "Install SAIGE R package\n",
    "\t\n",
    "```\n",
    "R \n",
    "devtools::install_github(\"weizhouUMICH/SAIGE\")`\n",
    "```\n",
    "\n",
    "\n",
    "Fixing problem with conda template: can't execute `conda activate` from bash script \n",
    "https://github.com/conda/conda/issues/7980\n",
    "\n",
    "Added these variables to `.bash_profile` apparently fixed the issue\n",
    "\n",
    "```\n",
    "export -f conda\n",
    "export -f __conda_activate\n",
    "export -f __conda_reactivate\n",
    "export -f __conda_hashr\n",
    "        \n",
    "```\n",
    "Then `source .bash_profile`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "kernel": "Bash"
   },
   "source": [
    "## d. SoS installation and configuration\n",
    "\n",
    "Install sos, sos-pbs and sos-notebook as the minimum requirements\n",
    "\n",
    "```\n",
    "conda install sos sos-pbs sos-notebook jupyterlab-sos sos-papermill -c conda-forge\n",
    "```\n",
    "\n",
    "Then check if all the kernels on jupyter are installed with this command : `$jupyter kernelspec list`, what we need are R, Bash and Python, if part of these kernels is missing, run this commmand to install: `$ conda install sos-r sos-python sos-bash -c conda-forge`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "kernel": "Bash"
   },
   "source": [
    "### SoS update to get the latest improvements\n",
    "\n",
    "For development versions\n",
    "\n",
    "```\n",
    "pip install git+https://github.com/vatlab/sos -U\n",
    "\n",
    "```\n",
    "\n",
    "For released versions (when is implemented)\n",
    "```\n",
    "pip install sos -U\n",
    "```\n",
    "\n",
    "When you don't get the full features of the update do:\n",
    "```\n",
    "pip uninstall sos\n",
    "pip install sos -U\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "kernel": "Bash"
   },
   "source": [
    "### Test your code before running it\n",
    "\n",
    "To check if the code in your notebook is running \n",
    "\n",
    "```\n",
    "sos dryrun notebook.ipynb -q localhost\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "kernel": "Bash"
   },
   "source": [
    "### SoS commands to create scripts from available notebooks\n",
    "\n",
    "Make sure you have the latest papermill version\n",
    "\n",
    "```\n",
    "pip install sos-papermill -U\n",
    "```\n",
    "\n",
    "```\n",
    "sos convert ~/project/pleiotropy_UKB/analysis/minimal_working_example.ipynb ~/project/pleiotropy_UKB/docs/analysis/minimal_working_example.html --execute\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "kernel": "Bash"
   },
   "source": [
    "## e. R installation using conda\n",
    "\n",
    "Intalling R with conda will allow you to manage your own packages. Refer to https://docs.ycrc.yale.edu/clusters-at-yale/guides/r/ for more information\n",
    "\n",
    "```\n",
    "conda install -c conda-forge r-base\n",
    "```\n",
    "To install R packages using conda \n",
    "\n",
    "```\n",
    "conda install -c r package_name\n",
    "\n",
    "e.g. packages required for LMM pipeline:\n",
    "conda install -c r qqman \n",
    "conda install -c r dplyr \n",
    "conda install -c r ggrepel \n",
    "conda install -c r ggplot2\n",
    "\n",
    "If this does not work, go to R to install:\n",
    "\n",
    "R\n",
    "install.packages(\"qqman\")\n",
    "install.packages(\"dplyr\")\n",
    "install.packages(\"ggrepel\")\n",
    "install.packages(\"ggplot2\")\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "kernel": "Bash"
   },
   "source": [
    "## f. QCTOOL version 2 usage\n",
    "\n",
    "If installed from source it requires zlib to be installed and compilation needs to be done with python 2 \n",
    "\n",
    "```\n",
    "cd ~/software && \\\n",
    "hg clone -r beta https://gavinband@bitbucket.org/gavinband/qctool && cd qctool\\\n",
    "./waf-1.5.18 configure --prefix=$MY_PREFIX  && ./waf-1.5.18 \\\n",
    "\n",
    "```\n",
    "\n",
    "If loaded from HPC cluster just write,\n",
    "\n",
    "```\n",
    "module load qctool\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "kernel": "Markdown"
   },
   "source": [
    "## h. REGENIE installation\n",
    "\n",
    "Requirements:\n",
    "* regenie requires compilation with GCC version >= 5.1 (on Linux) or Clang version >=3.3 (on Mac OSX). In the cluster you can load gcc by typing:\n",
    "```\n",
    "module load GCCcore/7.3.0\n",
    "```\n",
    "* Download and install [BGEN library](https://enkre.net/cgi-bin/code/bgen/dir?ci=trunk) to ~/software\n",
    "\n",
    "Steps:\n",
    "1. Clone the repo (make sure you compile the latest version)\n",
    "```\n",
    "git clone https://github.com/rgcgithub/regenie.git\n",
    "```\n",
    "2. In the source code edit the BGEN_PATH variable in the Makefile to the BGEN library path (/home/dc2325/software/bgen/) Remove space at the end\n",
    "3. On the command line type make while in the main source code directory.\n",
    "4. This should produce the executable called regenie\n",
    "\n",
    "Run the program,\n",
    "```\n",
    "./regenie --help\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "kernel": "SoS"
   },
   "source": [
    "If you find these problems try:\n",
    "\n",
    "regenie: /lib64/libstdc++.so.6: version `GLIBCXX_3.4.21' not found (required by regenie)\n",
    "regenie: /lib64/libstdc++.so.6: version `CXXABI_1.3.9' not found (required by regenie)\n",
    "regenie: /lib64/libstdc++.so.6: version `GLIBCXX_3.4.20' not found (required by regenie)\n",
    "\n",
    "```   \n",
    "conda install -c omgarcia gcc-6 # install GCC version 6\n",
    "conda install libgcc            # install conda gcc tools\n",
    "```\n",
    "make sure that you see GLIBCXX_3.4.xx on the list (which it could not find before)\n",
    "\n",
    "```\n",
    "strings miniconda3/lib/libstdc++.so.6 | grep GLIBCXX\n",
    "```\n",
    "\n",
    "add it to library paths\n",
    "\n",
    "```\n",
    "export LD_LIBRARY_PATH=<conda-env-path>/lib:$LD_LIBRARY_PATH\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "kernel": "SoS"
   },
   "source": [
    "## Using Singularity on Yale's cluster"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "kernel": "Markdown"
   },
   "source": [
    "Add this to your bash_profile or bashrc\n",
    "\n",
    "set SINGULARITY_CACHEDIR if you want to pull files (which can get big) somewhere other than $HOME/.singularity\n",
    "\n",
    "```\n",
    "export SINGULARITY_CACHEDIR=~/scratch60/.singularity\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "kernel": "Bash"
   },
   "source": [
    "# 3. SLURM commands on Yale's cluster\n",
    "\n",
    "Submit a submission script \n",
    "```\n",
    "sbatch <script>\n",
    "```\n",
    "\n",
    "List queued and running jobs\n",
    "```\n",
    "squeue -u$USER\n",
    "```\n",
    "\n",
    "Cancel a queued job or kill a running job\n",
    "```\n",
    "scancel <job_id>\n",
    "```\n",
    "\n",
    "Cancel all your jobs (running and pending)\n",
    "```\n",
    "scancel -u$USER\n",
    "```\n",
    "\n",
    "Check status of individual job (including failed or completed)\n",
    "```\n",
    "sacct -j <job_id>\n",
    "```\n",
    "\n",
    "To see all pending jobs sorted by priority (jobs with higher priority at the top)\n",
    "```\n",
    "squeue --sort=-p -t PD -p general\n",
    "```\n",
    "\n",
    "To cancel all pending jobs\n",
    "```\n",
    "scancel -t PENDING -u$USER\n",
    "```\n",
    "\n",
    "To see files that will be deleted from scratch60 (they purge every 30 days)\n",
    "```\n",
    "cat /gpfs/ysm/scratch60/todelete/${UID}\n",
    "```\n",
    "\n",
    "To see the last job submitted slurm\n",
    "```\n",
    "sacct\n",
    "sacct -S start-date -u user-name\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "kernel": "Bash"
   },
   "source": [
    "# 4. Starting a jupyter notebook server on Yale's cluster"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "kernel": "Bash"
   },
   "source": [
    "### 1. Submit/Start a jupyter-notebook server as a batch job. \n",
    "\n",
    "1.1. Clone the right github repo (containing the notebook to run), go the directory and run this command:`$ sbatch jupyter-tunnel.sh` to start the server, i.e. submit it as a job. \n",
    "\n",
    "1.2. Check if your job was submitted and is running with this command: `$ squeue -u$USER`  \n",
    "\n",
    "```\n",
    "Find the ST column:\n",
    "   R: indicates the job is running \n",
    "   PD: the job is pending (you will have to wait for it to start running, otherwise you could not find the information needed in the log file to connect, which will be expained in detail in step 2.1)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "kernel": "Bash"
   },
   "source": [
    "### 2. Start a ssh tunnel\n",
    "\n",
    "2.1. After making sure the job starts running, run command `$ ls` to find and open the log file. The name of it looks something like `jupyter-notebook-[jobid].log`. It contains all the information on how to connect. This will be located in the directory you submitted the job script from.\n",
    "\n",
    "\n",
    "Log file example: \n",
    "\n",
    "The content of your log file should look like as follow, find and grab the corresponding highlighted information from your log file, which will be used in the next steps:\n",
    "\n",
    "[hs863@farnam2 UKBB_GWAS_dev]$ cat jupyter-notebook-22823043.log\n",
    "\n",
    "For more info and how to connect from windows,\n",
    "   see https://docs.ycrc.yale.edu/clusters-at-yale/guides/jupyter/\n",
    "\n",
    "MacOS or linux terminal command to create your ssh tunnel\n",
    "`ssh -N -L 9240:c14n12:9240 hs863@farnam.hpc.yale.edu` --used in step 2.2\n",
    "\n",
    "Windows MobaXterm info\n",
    "Forwarded port:same as remote port\n",
    "Remote server: c14n12\n",
    "Remote port: 9240\n",
    "SSH server: farnam.hpc.yale.edu\n",
    "SSH login: hs863\n",
    "SSH port: 22\n",
    "\n",
    "Use a Browser on your local machine to go to:\n",
    "`localhost:9240  (prefix w/ https:// if using password)` --e.g. https://localhost:9240 -- used step 3.1\n",
    "\n",
    "[I 16:36:55.522 LabApp] Writing notebook server cookie secret to /gpfs/ysm/home/hs863/.local/share/jupyter/runtime/notebook_cookie_secret\n",
    "\n",
    "[I 16:37:02.326 LabApp] JupyterLab extension loaded from /gpfs/ysm/project/dewan/hs863/conda_envs/notebook_env/lib/python3.8/site-packages/jupyterlab\n",
    "\n",
    "[I 16:37:02.328 LabApp] JupyterLab application directory is /gpfs/ysm/project/dewan/hs863/conda_envs/notebook_env/share/jupyter/lab\n",
    "\n",
    "[I 16:37:02.331 LabApp] Serving notebooks from local directory: /gpfs/ysm/home/hs863\n",
    "\n",
    "[I 16:37:02.331 LabApp] The Jupyter Notebook is running at:\n",
    "\n",
    "[I 16:37:02.331 LabApp] http://c14n12:9240/?token=0454c9e908381e176a57ee92615b9d7ab07f00c20370b967 `(get the string after token=)` --used in step 3.2 / alternative url 1\n",
    "\n",
    "[I 16:37:02.331 LabApp]  or http://127.0.0.1:9240/?token=0454c9e908381e176a57ee92615b9d7ab07f00c20370b967 -- alternative url 2 of step3.1\n",
    "\n",
    "[I 16:37:02.331 LabApp] Use Control-C to stop this server and shut down all kernels (twice to skip confirmation).\n",
    "\n",
    "\n",
    "\n",
    "2.2 On a Mac or Linux machine, open a new terminal window and start the tunnel with `SSH` command. Get specific information from the log file."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "kernel": "Bash"
   },
   "source": [
    "### 3. Use local browser to connect and run the notebook\n",
    "\n",
    "3.1 Browse the notebook: open a browser in your local machine and enter the address `http://localhost:port` or you can use the alternative urls in the log file.\n",
    "\n",
    "3.2 Copy paste the token from the log file to the bar and enter.\n",
    "\n",
    "**Tip and Notes:** \n",
    "* The address Jupyter creates by default (the one with the name of a compute node) will not work outside the cluster's network. \n",
    "* The server is created for 24h. After that you will need a new one but the changes are saved to you folders. Also is better to exit when you are not using it in order to free computer resources."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "kernel": "Bash"
   },
   "source": [
    "## Accessing an interactive node\n",
    "\n",
    "Interactive jobs can be used for testing and troubleshooting code. By requesting an interactive job, you will be allocated resources and logged onto the node in a shell.\n",
    "\n",
    "```\n",
    "srun --pty -p interactive --mem-per-cpu=60G --cpus-per-task=1 --time=1-00:00:00 bash\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "kernel": "Bash"
   },
   "source": [
    "## Copying results to shared folder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "kernel": "Bash"
   },
   "source": [
    "After completing the runs of the association analysis you should copy the results to the project shared folder\n",
    "\n",
    "#### Path\n",
    "\n",
    "```\n",
    "/SAY/dbgapstg/scratch/UKBiobank/results/BOLTLMM_results/results_imputed_data/\n",
    "/SAY/dbgapstg/scratch/UKBiobank/results/FastGWA_results/results_imputed_data/\n",
    "```\n",
    "* The combined association analyses for the imputed data in .snp_stats.gz\n",
    "* The combined association analyses for the hard called genotypes in .stats.gz\n",
    "* The gziped .stderr files in .stats.stderr.gz\n",
    "* The gziped .stdout files in .stdout.gz"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "kernel": "Bash"
   },
   "source": [
    "# 5. Checking job failure on cluster\n",
    "\n",
    "When a job fails in the cluster make sure you do quadruple check to understand the origin of the error\n",
    "\n",
    "### Step 1\n",
    "Check the log file generated &> file.log that is located in the folder where you run the script\n",
    "\n",
    "### Step 2\n",
    "Check the stderr files that will be located in the output folder you choose\n",
    "\n",
    "### Step 3\n",
    "Check the .err files generated in the same folder where you run the script\n",
    "\n",
    "### Step 4\n",
    "Check the output of sos status task -v4\n",
    "\n",
    "Note: it is suggested that you remove all err and out files `rm *.err *.out`  when you decide to run another round"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "kernel": "SoS"
   },
   "source": [
    "# 6. Purging traces after runs\n",
    "\n",
    "First of all, make sure there are not running pipelines.\n",
    "Then, under the working directory (the place where you run the sbatch scripts) run the following commands to remove any traces of previous runs\n",
    "\n",
    "```\n",
    "rm -rf ~/.sos\n",
    "rm -rf .sos\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "kernel": "SoS"
   },
   "source": [
    "# 7. Accessing Columbia cluster\n",
    "\n",
    "First of all you need to set a password by entering this website\n",
    "\n",
    "https://portal.neuro.columbia.edu/vpn/index.html\n",
    "\n",
    "```\n",
    "uni: dmc2245\n",
    "Password: Neurology99 is originated by default\n",
    "```\n",
    "\n",
    "You will need to change it when it is the first time\n",
    "\n",
    "Then using AnyConnect type in the vpn:\n",
    "\n",
    "```\n",
    "ssl.cpmc.columbia.edu\n",
    "select CUMC-VPN\n",
    "uni: dmc2245\n",
    "password: it should be the same as myColumbia (cas.columbia)\n",
    "push (to verify with duo mobile)\n",
    "```\n",
    "After this go to the terminal and ssh into the cluster\n",
    "\n",
    "```\n",
    "ssh dmc2245@hgrcgrid.cpmc.columbia.edu\n",
    "password (it is different from the VPN password)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "kernel": "SoS"
   },
   "source": [
    "# 8. Syncing the results between clusters\n",
    "```\n",
    "rsync -auzP /from/this/yale/path/* /to/this/columbia/path\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "kernel": "SoS"
   },
   "source": [
    "# 9. Building an image using singularity\n",
    "\n",
    "Yale's cluster\n",
    "\n",
    "```\n",
    "   singularity remote login\n",
    "   singularity build --remote  marp.sif Singularity\n",
    "   singularity shell -B /tmp:/scratch marp.sif\n",
    "```\n",
    "```\n",
    "   \n",
    "   singularity build lmm.sif docker://statisticalgenetics/lmm:1.9\n",
    "   singularity build annovar.sif docker://gaow/gatk4-annovar\n",
    "```\n",
    "\n",
    "Columbia's cluster\n",
    "\n",
    "```\n",
    "module load Singularity/3.5.3\n",
    "singularity remote login\n",
    "> enter your token\n",
    "singularity build --remote lmm.sif docker://statisticalgenetics/lmm:2.2\n",
    "```\n",
    "## Using singularity on the cluster\n",
    "\n",
    "```\n",
    "singularity shell --shell /bin/bash /mnt/mfs/statgen/containers/lmm.sif\n",
    "\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "kernel": "SoS"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "kernel": "SoS"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "SoS",
   "language": "sos",
   "name": "sos"
  },
  "language_info": {
   "codemirror_mode": "sos",
   "file_extension": ".sos",
   "mimetype": "text/x-sos",
   "name": "sos",
   "nbconvert_exporter": "sos_notebook.converter.SoS_Exporter",
   "pygments_lexer": "sos"
  },
  "sos": {
   "kernels": [
    [
     "Markdown",
     "markdown",
     "markdown",
     "",
     ""
    ]
   ],
   "version": "0.22.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
